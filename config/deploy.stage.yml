# Name of your application. Used to uniquely configure containers.
service: venture_verse_mini_app_rails

# Name of the container image.
image: brincmafai/venture_verse_mini_app_rails

# Deploy to these servers.
servers:
  web:
    - 172.105.202.16
  cron:
    hosts:
      - 172.105.202.16
    cmd:
      bash -c "(env && cat config/crontab) | crontab - && cron -f"
    options:
      user: root
  job:
    hosts:
      - 172.105.202.16
    cmd: bin/jobs

# Proxy configuration for staging
proxy:
  ssl: false
  app_port: 3000
  host: stageminiapp.ventureverse.com

registry:
  # Specify the registry server, if you're not using Docker Hub
  # server: registry.digitalocean.com / ghcr.io / ...
  username: brincmafai

  # Always use an access token rather than real password when possible.
  password:
    - KAMAL_REGISTRY_PASSWORD

# Inject ENV variables into containers (secrets come from .kamal/secrets).
env:
  secret:
    - RAILS_MASTER_KEY
  clear:
    # Set Rails environment to staging
    RAILS_ENV: staging
    
    # Run the Solid Queue Supervisor inside the web server's Puma process to do jobs.
    # When you start using multiple servers, you should split out job processing to a dedicated machine.
    SOLID_QUEUE_IN_PUMA: true

# Use a persistent storage volume for sqlite database files and local Active Storage files.
# Use different volume name for staging to avoid conflicts with production.
volumes:
  - "venture_verse_mini_app_rails_storage:/rails/storage"

# Bridge fingerprinted assets, like JS and CSS, between versions to avoid
# hitting 404 on in-flight requests. Combines all files from new and old
# version inside the asset_path.
asset_path: /rails/public/assets

# Configure the image builder.
builder:
  arch: amd64
